{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import random as rand\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = dsets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = dsets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    avDev = torch.device(\"cuda\")\n",
    "else:\n",
    "    avDev = torch.device(\"cpu\")\n",
    "\n",
    "print(avDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MAKING DATASET ITERABLE\n",
    " \n",
    "batch_size = 500\n",
    "n_iters = 5000\n",
    "num_epochs = n_iters / (len(trainset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    " \n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, \n",
    "                                          batch_size=len(testset), \n",
    "                                          shuffle=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation_fn):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 3000)\n",
    "        self.linear1_drop = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(3000, 3000)\n",
    "        self.linear2_drop = nn.Dropout(0.2)\n",
    "        self.linear3 = nn.Linear(3000, output_dim)\n",
    "        self.activation_fn = activation_fn\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.activation_fn(self.linear1(x))\n",
    "        layer1_out = self.linear1_drop(out)\n",
    "        out = self.activation_fn(self.linear2(layer1_out))\n",
    "        layer2_out = self.linear2_drop(out)\n",
    "        out = self.linear3(layer2_out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss().to(avDev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers = [torch.optim.SGD, optimizer_Adam, optimizer_Adagrad, optimizer_Adadelta, optimizer_RMSprop]\n",
    "\n",
    "optimizers = ['SGD','Adam','Adagrad', 'Adadelta', 'RMSprop']\n",
    "activation_fns = [nn.ReLU(), nn.Tanh(), nn.Sigmoid()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1, and L2 regularization parms\n",
    "\n",
    "lambda1 = 0.001\n",
    "lambda2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "def train_model():\n",
    "    for activation_fn in activation_fns:\n",
    "        \n",
    "        input_dim = 3*32*32\n",
    "        output_dim = 10\n",
    "        \n",
    "        model = None\n",
    "        model = LogisticRegressionModel(input_dim, output_dim, activation_fn)\n",
    "        model.to(avDev)\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "            \n",
    "            print('Activation Function: {}. Optimizer: {}'.format(activation_fn,optimizer))\n",
    "                  \n",
    "            if optimizer == 'SGD':\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "                \n",
    "            elif optimizer == 'Adam':\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "                \n",
    "            elif optimizer == 'Adagrad':\n",
    "                optimizer = torch.optim.Adagrad(model.parameters(), lr=0.0003)\n",
    "                \n",
    "            elif optimizer == 'Adadelta':\n",
    "                optimizer = torch.optim.Adadelta(model.parameters(), lr=0.0003)\n",
    "                \n",
    "            elif optimizer == 'RMSprop':\n",
    "                optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0003)\n",
    "\n",
    "            loss_save = np.empty(n_iters-1)\n",
    "            iter = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "                    images = images.view(-1, 3*32*32).to(avDev)\n",
    "                    labels = labels.to(avDev)\n",
    "\n",
    "                    # Clear gradients w.r.t. parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                    # Forward pass to get output/logits\n",
    "                    outputs = model(images)\n",
    "            #         outputs = model(images)\n",
    "\n",
    "                    # Calculate Loss: softmax --> cross entropy loss\n",
    "                    cross_entropy_loss = criterion(outputs, labels)#\n",
    "\n",
    "\n",
    "                    all_linear1_params = torch.cat([x.view(-1) for x in model.linear1.parameters()])\n",
    "                    all_linear2_params = torch.cat([x.view(-1) for x in model.linear2.parameters()])\n",
    "                    l1_regularization = lambda1 * torch.norm(all_linear1_params, 1)\n",
    "                    l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)\n",
    "\n",
    "                    loss = cross_entropy_loss\n",
    "                    # loss = cross_entropy_loss + l1regularization + l2_regularization\n",
    "                    loss = cross_entropy_loss + l2_regularization\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                    #Save Loss    \n",
    "\n",
    "                    loss_save[iter-1] = loss.item()\n",
    "                    iter += 1\n",
    "\n",
    "                    if iter % 200 == 0:\n",
    "                        # Calculate Accuracy         \n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "                        # Iterate through test dataset\n",
    "                        for images, labels in test_loader:\n",
    "                            #######################\n",
    "                            #  USE GPU FOR MODEL  #\n",
    "                            #######################\n",
    "                            images = images.view(-1, 3*32*32).to(avDev)\n",
    "\n",
    "                            # Forward pass only to get logits/output\n",
    "                            outputs = model(images)\n",
    "\n",
    "                            # Get predictions from the maximum value\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                            # Total number of labels\n",
    "                            total += labels.size(0)\n",
    "\n",
    "                            #######################\n",
    "                            #  USE GPU FOR MODEL  #\n",
    "                            #######################\n",
    "                            # Total correct predictions\n",
    "                            correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
    "\n",
    "                        accuracy = 100. * correct / total\n",
    "\n",
    "                        # Print Loss\n",
    "                        print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: ReLU(). Optimizer: SGD\n",
      "Iteration: 200. Loss: 5.036365032196045. Accuracy: 30.81999969482422\n",
      "Iteration: 400. Loss: 4.907623767852783. Accuracy: 34.81999969482422\n",
      "Iteration: 600. Loss: 4.817268371582031. Accuracy: 37.209999084472656\n",
      "Iteration: 800. Loss: 4.669703006744385. Accuracy: 38.68000030517578\n",
      "Iteration: 1000. Loss: 4.5388007164001465. Accuracy: 40.779998779296875\n",
      "Iteration: 1200. Loss: 4.395656585693359. Accuracy: 42.029998779296875\n",
      "Iteration: 1400. Loss: 4.325504779815674. Accuracy: 43.2599983215332\n",
      "Iteration: 1600. Loss: 4.301024436950684. Accuracy: 42.119998931884766\n",
      "Iteration: 1800. Loss: 4.226015567779541. Accuracy: 44.83000183105469\n",
      "Iteration: 2000. Loss: 4.133245944976807. Accuracy: 44.650001525878906\n",
      "Iteration: 2200. Loss: 4.029412269592285. Accuracy: 44.720001220703125\n",
      "Iteration: 2400. Loss: 4.002126693725586. Accuracy: 43.97999954223633\n",
      "Iteration: 2600. Loss: 3.946878433227539. Accuracy: 44.59000015258789\n",
      "Iteration: 2800. Loss: 3.8098902702331543. Accuracy: 47.54999923706055\n",
      "Iteration: 3000. Loss: 3.8738491535186768. Accuracy: 46.66999816894531\n",
      "Iteration: 3200. Loss: 3.6921894550323486. Accuracy: 48.349998474121094\n",
      "Iteration: 3400. Loss: 3.6951096057891846. Accuracy: 47.59000015258789\n",
      "Iteration: 3600. Loss: 3.48299503326416. Accuracy: 48.45000076293945\n",
      "Iteration: 3800. Loss: 3.445495128631592. Accuracy: 47.5\n",
      "Iteration: 4000. Loss: 3.31632661819458. Accuracy: 50.349998474121094\n",
      "Iteration: 4200. Loss: 3.382718324661255. Accuracy: 49.099998474121094\n",
      "Iteration: 4400. Loss: 3.248298168182373. Accuracy: 49.20000076293945\n",
      "Iteration: 4600. Loss: 3.2000508308410645. Accuracy: 48.15999984741211\n",
      "Iteration: 4800. Loss: 3.119443893432617. Accuracy: 50.630001068115234\n",
      "Iteration: 5000. Loss: 2.98148775100708. Accuracy: 51.06999969482422\n",
      "Activation Function: ReLU(). Optimizer: Adam\n",
      "Iteration: 200. Loss: 2.0607638359069824. Accuracy: 46.84000015258789\n",
      "Iteration: 400. Loss: 1.7127933502197266. Accuracy: 47.599998474121094\n",
      "Iteration: 600. Loss: 1.697632074356079. Accuracy: 48.59000015258789\n",
      "Iteration: 800. Loss: 1.6403803825378418. Accuracy: 48.16999816894531\n",
      "Iteration: 1000. Loss: 1.6463687419891357. Accuracy: 46.75\n",
      "Iteration: 1200. Loss: 1.6205153465270996. Accuracy: 49.02000045776367\n",
      "Iteration: 1400. Loss: 1.5538007020950317. Accuracy: 49.0\n",
      "Iteration: 1600. Loss: 1.5431442260742188. Accuracy: 48.52000045776367\n",
      "Iteration: 1800. Loss: 1.6091774702072144. Accuracy: 49.5099983215332\n",
      "Iteration: 2000. Loss: 1.4048908948898315. Accuracy: 49.59000015258789\n",
      "Iteration: 2200. Loss: 1.4575128555297852. Accuracy: 50.20000076293945\n",
      "Iteration: 2400. Loss: 1.4775099754333496. Accuracy: 50.15999984741211\n",
      "Iteration: 2600. Loss: 1.5410895347595215. Accuracy: 51.29999923706055\n",
      "Iteration: 2800. Loss: 1.5397484302520752. Accuracy: 50.970001220703125\n",
      "Iteration: 3000. Loss: 1.5254664421081543. Accuracy: 50.900001525878906\n",
      "Iteration: 3200. Loss: 1.5053142309188843. Accuracy: 51.83000183105469\n",
      "Iteration: 3400. Loss: 1.3535411357879639. Accuracy: 51.20000076293945\n",
      "Iteration: 3600. Loss: 1.4658489227294922. Accuracy: 51.31999969482422\n",
      "Iteration: 3800. Loss: 1.3902939558029175. Accuracy: 52.25\n",
      "Iteration: 4000. Loss: 1.4521875381469727. Accuracy: 51.58000183105469\n",
      "Iteration: 4200. Loss: 1.391920804977417. Accuracy: 52.349998474121094\n",
      "Iteration: 4400. Loss: 1.3269679546356201. Accuracy: 52.189998626708984\n",
      "Iteration: 4600. Loss: 1.3900341987609863. Accuracy: 51.849998474121094\n",
      "Iteration: 4800. Loss: 1.4702517986297607. Accuracy: 52.650001525878906\n",
      "Iteration: 5000. Loss: 1.4185688495635986. Accuracy: 52.099998474121094\n",
      "Activation Function: ReLU(). Optimizer: Adagrad\n",
      "Iteration: 200. Loss: 1.2262169122695923. Accuracy: 55.029998779296875\n",
      "Iteration: 400. Loss: 1.225508451461792. Accuracy: 54.75\n",
      "Iteration: 600. Loss: 1.176718831062317. Accuracy: 54.779998779296875\n",
      "Iteration: 800. Loss: 1.2106720209121704. Accuracy: 54.33000183105469\n",
      "Iteration: 1000. Loss: 1.0990239381790161. Accuracy: 54.72999954223633\n",
      "Iteration: 1200. Loss: 1.2208985090255737. Accuracy: 54.81999969482422\n",
      "Iteration: 1400. Loss: 1.2375277280807495. Accuracy: 54.59000015258789\n",
      "Iteration: 1600. Loss: 1.180375576019287. Accuracy: 54.38999938964844\n",
      "Iteration: 1800. Loss: 1.1195913553237915. Accuracy: 54.77000045776367\n",
      "Iteration: 2000. Loss: 1.2319575548171997. Accuracy: 54.529998779296875\n",
      "Iteration: 2200. Loss: 1.1639504432678223. Accuracy: 55.02000045776367\n",
      "Iteration: 2400. Loss: 1.1702035665512085. Accuracy: 54.77000045776367\n",
      "Iteration: 2600. Loss: 1.216522455215454. Accuracy: 55.540000915527344\n",
      "Iteration: 2800. Loss: 1.0842266082763672. Accuracy: 54.900001525878906\n",
      "Iteration: 3000. Loss: 1.160678744316101. Accuracy: 54.880001068115234\n",
      "Iteration: 3200. Loss: 1.2041130065917969. Accuracy: 55.75\n",
      "Iteration: 3400. Loss: 1.2380218505859375. Accuracy: 54.81999969482422\n",
      "Iteration: 3600. Loss: 1.1910570859909058. Accuracy: 55.349998474121094\n",
      "Iteration: 3800. Loss: 1.14095938205719. Accuracy: 54.63999938964844\n",
      "Iteration: 4000. Loss: 1.2188096046447754. Accuracy: 54.58000183105469\n",
      "Iteration: 4200. Loss: 1.2282803058624268. Accuracy: 54.75\n",
      "Iteration: 4400. Loss: 1.2390239238739014. Accuracy: 54.529998779296875\n",
      "Iteration: 4600. Loss: 1.1827229261398315. Accuracy: 54.81999969482422\n",
      "Iteration: 4800. Loss: 1.103094220161438. Accuracy: 54.939998626708984\n",
      "Iteration: 5000. Loss: 1.20871901512146. Accuracy: 55.22999954223633\n",
      "Activation Function: ReLU(). Optimizer: Adadelta\n",
      "Iteration: 200. Loss: 1.1986063718795776. Accuracy: 54.970001220703125\n",
      "Iteration: 400. Loss: 1.18323814868927. Accuracy: 54.83000183105469\n",
      "Iteration: 600. Loss: 1.2318997383117676. Accuracy: 54.68000030517578\n",
      "Iteration: 800. Loss: 1.161531925201416. Accuracy: 55.2599983215332\n",
      "Iteration: 1000. Loss: 1.2278034687042236. Accuracy: 55.43000030517578\n",
      "Iteration: 1200. Loss: 1.1538640260696411. Accuracy: 54.97999954223633\n",
      "Iteration: 1400. Loss: 1.1084152460098267. Accuracy: 54.810001373291016\n",
      "Iteration: 1600. Loss: 1.1654871702194214. Accuracy: 55.310001373291016\n",
      "Iteration: 1800. Loss: 1.1402887105941772. Accuracy: 55.130001068115234\n",
      "Iteration: 2000. Loss: 1.1246005296707153. Accuracy: 54.959999084472656\n",
      "Iteration: 2200. Loss: 1.201185941696167. Accuracy: 54.77000045776367\n",
      "Iteration: 2400. Loss: 1.1276715993881226. Accuracy: 55.0099983215332\n",
      "Iteration: 2600. Loss: 1.160884976387024. Accuracy: 55.59000015258789\n",
      "Iteration: 2800. Loss: 1.1226835250854492. Accuracy: 55.31999969482422\n",
      "Iteration: 3000. Loss: 1.1935029029846191. Accuracy: 54.90999984741211\n",
      "Iteration: 3200. Loss: 1.202795147895813. Accuracy: 54.779998779296875\n",
      "Iteration: 3400. Loss: 1.0958462953567505. Accuracy: 55.04999923706055\n",
      "Iteration: 3600. Loss: 1.1734726428985596. Accuracy: 55.45000076293945\n",
      "Iteration: 3800. Loss: 1.175544023513794. Accuracy: 55.439998626708984\n",
      "Iteration: 4000. Loss: 1.2065949440002441. Accuracy: 55.20000076293945\n",
      "Iteration: 4200. Loss: 1.2150275707244873. Accuracy: 55.06999969482422\n",
      "Iteration: 4400. Loss: 1.260068655014038. Accuracy: 55.11000061035156\n",
      "Iteration: 4600. Loss: 1.0975062847137451. Accuracy: 54.869998931884766\n",
      "Iteration: 4800. Loss: 1.1579967737197876. Accuracy: 55.20000076293945\n",
      "Iteration: 5000. Loss: 1.1037280559539795. Accuracy: 55.150001525878906\n",
      "Activation Function: ReLU(). Optimizer: RMSprop\n",
      "Iteration: 200. Loss: 1.6672649383544922. Accuracy: 47.220001220703125\n",
      "Iteration: 400. Loss: 1.7495810985565186. Accuracy: 48.040000915527344\n",
      "Iteration: 600. Loss: 1.6232414245605469. Accuracy: 48.54999923706055\n",
      "Iteration: 800. Loss: 1.4885529279708862. Accuracy: 49.86000061035156\n",
      "Iteration: 1000. Loss: 1.63437020778656. Accuracy: 45.279998779296875\n",
      "Iteration: 1200. Loss: 1.5293278694152832. Accuracy: 48.349998474121094\n",
      "Iteration: 1400. Loss: 1.3674180507659912. Accuracy: 49.310001373291016\n",
      "Iteration: 1600. Loss: 1.4153263568878174. Accuracy: 50.630001068115234\n",
      "Iteration: 1800. Loss: 1.5952285528182983. Accuracy: 48.130001068115234\n",
      "Iteration: 2000. Loss: 1.407735824584961. Accuracy: 51.2400016784668\n",
      "Iteration: 2200. Loss: 1.9335471391677856. Accuracy: 43.209999084472656\n",
      "Iteration: 2400. Loss: 1.4225072860717773. Accuracy: 51.06999969482422\n",
      "Iteration: 2600. Loss: 1.4089670181274414. Accuracy: 47.810001373291016\n",
      "Iteration: 2800. Loss: 1.4871044158935547. Accuracy: 49.09000015258789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3000. Loss: 1.5556707382202148. Accuracy: 50.150001525878906\n",
      "Iteration: 3200. Loss: 1.418527603149414. Accuracy: 49.27000045776367\n",
      "Iteration: 3400. Loss: 1.5757226943969727. Accuracy: 50.31999969482422\n",
      "Iteration: 3600. Loss: 1.4734660387039185. Accuracy: 50.849998474121094\n",
      "Iteration: 3800. Loss: 1.3816049098968506. Accuracy: 49.59000015258789\n",
      "Iteration: 4000. Loss: 1.4171044826507568. Accuracy: 48.970001220703125\n",
      "Iteration: 4200. Loss: 1.4374744892120361. Accuracy: 50.63999938964844\n",
      "Iteration: 4400. Loss: 1.4622917175292969. Accuracy: 46.91999816894531\n",
      "Iteration: 4600. Loss: 1.3940668106079102. Accuracy: 51.5\n",
      "Iteration: 4800. Loss: 1.386926531791687. Accuracy: 49.36000061035156\n",
      "Iteration: 5000. Loss: 1.2961565256118774. Accuracy: 51.15999984741211\n",
      "Activation Function: Tanh(). Optimizer: SGD\n",
      "Iteration: 200. Loss: 5.020430088043213. Accuracy: 34.77000045776367\n",
      "Iteration: 400. Loss: 4.909233093261719. Accuracy: 36.0\n",
      "Iteration: 600. Loss: 4.730772018432617. Accuracy: 37.720001220703125\n",
      "Iteration: 800. Loss: 4.744655609130859. Accuracy: 39.22999954223633\n",
      "Iteration: 1000. Loss: 4.613026142120361. Accuracy: 39.11000061035156\n",
      "Iteration: 1200. Loss: 4.5372114181518555. Accuracy: 38.34000015258789\n",
      "Iteration: 1400. Loss: 4.449986934661865. Accuracy: 39.31999969482422\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.to(avDev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 3*32*32).to(avDev)\n",
    "        labels = labels.to(avDev)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the  train images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data = loss_save)\n",
    "ax.set(xlabel = \"Iterations\", ylabel=\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(avDev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 3*32*32).to(avDev)\n",
    "        labels = labels.to(avDev)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "cm = confusion_matrix(predicted.cpu(), labels.cpu()).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "plt.figure(figsize=(12,10))\n",
    "df_cm = pd.DataFrame(cm,CLASSES,CLASSES)\n",
    "sns.set(font_scale=1.4)\n",
    "ax = sns.heatmap(df_cm,annot=True,fmt='d')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
