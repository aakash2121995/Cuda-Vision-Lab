{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import random as rand\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = dsets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = dsets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    avDev = torch.device(\"cuda\")\n",
    "else:\n",
    "    avDev = torch.device(\"cpu\")\n",
    "\n",
    "print(avDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MAKING DATASET ITERABLE\n",
    " \n",
    "batch_size = 500\n",
    "n_iters = 5000\n",
    "num_epochs = n_iters / (len(trainset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    " \n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, \n",
    "                                          batch_size=len(testset), \n",
    "                                          shuffle=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation_fn):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 3000)\n",
    "        self.linear1_drop = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(3000, 3000)\n",
    "        self.linear2_drop = nn.Dropout(0.2)\n",
    "        self.linear3 = nn.Linear(3000, output_dim)\n",
    "        self.activation_fn = activation_fn\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.activation_fn(self.linear1(x))\n",
    "        layer1_out = self.linear1_drop(out)\n",
    "        out = self.activation_fn(self.linear2(layer1_out))\n",
    "        layer2_out = self.linear2_drop(out)\n",
    "        out = self.linear3(layer2_out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 3*32*32\n",
    "output_dim = 10\n",
    " \n",
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    " \n",
    "model.to(avDev)\n",
    " \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss().to(avDev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.03\n",
    " \n",
    "optimizer_SGD = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_Adam = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_Adagrad = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_Adadelta = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_RMSprop = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel(\n",
      "  (linear1): Linear(in_features=3072, out_features=3000, bias=True)\n",
      "  (linear1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (linear2): Linear(in_features=3000, out_features=3000, bias=True)\n",
      "  (linear2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (linear3): Linear(in_features=3000, out_features=10, bias=True)\n",
      "  (activation_fn): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1, and L2 regularization parms\n",
    "\n",
    "lambda1 = 0.001\n",
    "lambda2 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "def train_model(optimizer):\n",
    "    loss_save = np.empty(n_iters-1)\n",
    "\n",
    "    iter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            images = images.view(-1, 3*32*32).to(avDev)\n",
    "            labels = labels.to(avDev)\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(images)\n",
    "    #         outputs = model(images)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            cross_entropy_loss = criterion(outputs, labels)#\n",
    "\n",
    "\n",
    "            all_linear1_params = torch.cat([x.view(-1) for x in model.linear1.parameters()])\n",
    "            all_linear2_params = torch.cat([x.view(-1) for x in model.linear2.parameters()])\n",
    "            l1_regularization = lambda1 * torch.norm(all_linear1_params, 1)\n",
    "            l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)\n",
    "\n",
    "#             loss = cross_entropy_loss + l1_regularization + l2_regularization\n",
    "\n",
    "            loss = cross_entropy_loss + l2_regularization\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            #Save Loss    \n",
    "\n",
    "            loss_save[iter-1] = loss.item()\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 200 == 0:\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for images, labels in test_loader:\n",
    "                    #######################\n",
    "                    #  USE GPU FOR MODEL  #\n",
    "                    #######################\n",
    "                    images = images.view(-1, 3*32*32).to(avDev)\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    #######################\n",
    "                    #  USE GPU FOR MODEL  #\n",
    "                    #######################\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
    "\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200. Loss: 2.626703977584839. Accuracy: 10.449999809265137\n",
      "Iteration: 400. Loss: 2.6186680793762207. Accuracy: 10.380000114440918\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Tanh())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Sigmoid())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Tanh())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Sigmoid())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Tanh())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Sigmoid())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Tanh())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Sigmoid())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.ReLU())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Tanh())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim, output_dim, nn.Sigmoid())\n",
    "model.to(avDev)\n",
    "train_model(optimizer_RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.to(avDev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 3*32*32).to(avDev)\n",
    "        labels = labels.to(avDev)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the  train images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data = loss_save)\n",
    "ax.set(xlabel = \"Iterations\", ylabel=\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(avDev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 3*32*32).to(avDev)\n",
    "        labels = labels.to(avDev)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "cm = confusion_matrix(predicted.cpu(), labels.cpu()).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "plt.figure(figsize=(12,10))\n",
    "df_cm = pd.DataFrame(cm,CLASSES,CLASSES)\n",
    "sns.set(font_scale=1.4)\n",
    "ax = sns.heatmap(df_cm,annot=True,fmt='d')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
